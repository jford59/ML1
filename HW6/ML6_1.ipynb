{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtRZ6K2UYAZ/zV2qsP2qqI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jford59/ML1/blob/main/HW6/ML6_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFRGiKnELiVL",
        "outputId": "4e4d8fda-7b9b-4f61-9588-1072062971e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-cf8441e7ed03>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  t_u = torch.tensor(X)\n",
            "<ipython-input-29-cf8441e7ed03>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  t_c = torch.tensor(y)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([436])) that is different to the input size (torch.Size([436, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([109])) that is different to the input size (torch.Size([109, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training loss 0.0345, Validation loss 0.0282\n",
            "Epoch 1000, Training loss 0.0330, Validation loss 0.0265\n",
            "Epoch 1500, Training loss 0.0319, Validation loss 0.0253\n",
            "Epoch 2000, Training loss 0.0311, Validation loss 0.0245\n",
            "Epoch 2500, Training loss 0.0305, Validation loss 0.0239\n",
            "Epoch 3000, Training loss 0.0301, Validation loss 0.0234\n",
            "Epoch 3500, Training loss 0.0297, Validation loss 0.0230\n",
            "Epoch 4000, Training loss 0.0295, Validation loss 0.0227\n",
            "Epoch 4500, Training loss 0.0292, Validation loss 0.0225\n",
            "Epoch 5000, Training loss 0.0290, Validation loss 0.0223\n",
            "Epoch 500, Training loss 0.1548, Validation loss 0.1512\n",
            "Epoch 1000, Training loss 0.1548, Validation loss 0.1512\n",
            "Epoch 1500, Training loss 0.1548, Validation loss 0.1512\n",
            "Epoch 2000, Training loss 0.1548, Validation loss 0.1512\n",
            "Epoch 2500, Training loss 0.1548, Validation loss 0.1512\n",
            "Epoch 3000, Training loss 0.1548, Validation loss 0.1512\n",
            "Epoch 3500, Training loss 0.1548, Validation loss 0.1512\n",
            "Epoch 4000, Training loss 0.1548, Validation loss 0.1512\n",
            "Epoch 4500, Training loss 0.1548, Validation loss 0.1512\n",
            "Epoch 5000, Training loss 0.1548, Validation loss 0.1512\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/My Drive/Housing.csv'\n",
        "housing = pd.DataFrame(pd.read_csv(file_path))\n",
        "\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
        "\n",
        "varlist =  ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']\n",
        "charlist = ['mainroad','guestroom','basement','hotwaterheating','airconditioning','prefarea']\n",
        "\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "housing[varlist] = scaler.fit_transform(housing[varlist])\n",
        "\n",
        "t_u = torch.tensor(X)\n",
        "t_c = torch.tensor(y)\n",
        "def binary_map(x):\n",
        "  return x.map({'yes' : 1, 'no' : 0})\n",
        "housing[charlist] = housing[charlist].apply(binary_map)\n",
        "\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "housing[varlist] = scaler.fit_transform(housing[varlist])\n",
        "\n",
        "housing.pop('furnishingstatus')\n",
        "x = housing.values\n",
        "y = housing.pop('price')\n",
        "\n",
        "X = torch.tensor(x)\n",
        "y = torch.tensor(y)\n",
        "params.grad is None\n",
        "n_samples = x.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "\n",
        "X_train = X[train_indices]\n",
        "y_train = y[train_indices]\n",
        "\n",
        "X_test = X[val_indices]\n",
        "y_test = y[val_indices]\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, X_train, X_test, y_train, y_test):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    t_p_train = model(X_train) # <1>\n",
        "    loss_train = loss_fn(t_p_train, y_train)\n",
        "\n",
        "    t_p_test = model(X_test) # <1>\n",
        "    loss_val = loss_fn(t_p_test, y_test)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss_train.backward() # <2>\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0 :\n",
        "      print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\" f\" Validation loss {loss_val.item():.4f}\")\n",
        "\n",
        "def loss_fn(t_p, y):\n",
        "    squared_diffs = (t_p - t_c)**2\n",
        "    return squared_diffs.mean()\n",
        "\n",
        "neuron_count = 32\n",
        "\n",
        "seq_model = nn.Sequential(OrderedDict([\n",
        "    ('hidden_linear', nn.Linear(12, neuron_count)),\n",
        "    ('hidden_activation', nn.Tanh()),\n",
        "    ('output_linear', nn.Linear(neuron_count, 1))\n",
        "]))\n",
        "\n",
        "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3) # <1>\n",
        "\n",
        "#1 layer\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    model = seq_model,\n",
        "    loss_fn = nn.MSELoss(),\n",
        "    X_train = X_train.to(torch.float32),\n",
        "    X_test = X_test.to(torch.float32),\n",
        "    y_train = y_train.to(torch.float32),\n",
        "    y_test = y_test.to(torch.float32)\n",
        "    )\n",
        "\n",
        "seq_model = nn.Sequential(OrderedDict([\n",
        "    ('hidden_linear', nn.Linear(12, 32)),\n",
        "    ('hidden_activation', nn.Tanh()),\n",
        "    ('hidden_linear1', nn.Linear(32, 64)),\n",
        "    ('hidden_activation1', nn.Tanh()),\n",
        "    ('hidden_linear2', nn.Linear(64, 16)),\n",
        "    ('hidden_activation2', nn.Tanh()),\n",
        "    ('output_linear', nn.Linear(16, 1))\n",
        "]))\n",
        "\n",
        "#3 layers\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    model = seq_model,\n",
        "    loss_fn = loss_fn,\n",
        "    X_train = X_train.to(torch.float32),\n",
        "    X_test = X_test.to(torch.float32),\n",
        "    y_train = y_train.to(torch.float32),\n",
        "    y_test = y_test.to(torch.float32))\n"
      ]
    }
  ]
}