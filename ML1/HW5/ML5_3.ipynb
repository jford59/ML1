{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUSSIihXG0wp/QVPAeOGpw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jford59/ML1/blob/main/ML1/HW5/ML5_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jBVbZY8MakL",
        "outputId": "50999824-b212-480f-cdb0-9e40ea0f2f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 500, Training loss 0.0120, Validation loss 0.0112\n",
            "Epoch 1000, Training loss 0.0113, Validation loss 0.0103\n",
            "Epoch 1500, Training loss 0.0108, Validation loss 0.0098\n",
            "Epoch 2000, Training loss 0.0105, Validation loss 0.0095\n",
            "Epoch 2500, Training loss 0.0103, Validation loss 0.0092\n",
            "Epoch 3000, Training loss 0.0102, Validation loss 0.0091\n",
            "Epoch 3500, Training loss 0.0101, Validation loss 0.0090\n",
            "Epoch 4000, Training loss 0.0100, Validation loss 0.0089\n",
            "Epoch 4500, Training loss 0.0100, Validation loss 0.0088\n",
            "Epoch 5000, Training loss 0.0099, Validation loss 0.0087\n",
            "Epoch 500, Training loss 0.0131, Validation loss 0.0125\n",
            "Epoch 1000, Training loss 0.0130, Validation loss 0.0123\n",
            "Epoch 1500, Training loss 0.0128, Validation loss 0.0121\n",
            "Epoch 2000, Training loss 0.0127, Validation loss 0.0120\n",
            "Epoch 2500, Training loss 0.0126, Validation loss 0.0118\n",
            "Epoch 3000, Training loss 0.0124, Validation loss 0.0117\n",
            "Epoch 3500, Training loss 0.0123, Validation loss 0.0115\n",
            "Epoch 4000, Training loss 0.0122, Validation loss 0.0114\n",
            "Epoch 4500, Training loss 0.0121, Validation loss 0.0113\n",
            "Epoch 5000, Training loss 0.0120, Validation loss 0.0112\n",
            "Epoch 500, Training loss 0.0137, Validation loss 0.0135\n",
            "Epoch 1000, Training loss 0.0133, Validation loss 0.0129\n",
            "Epoch 1500, Training loss 0.0133, Validation loss 0.0127\n",
            "Epoch 2000, Training loss 0.0132, Validation loss 0.0126\n",
            "Epoch 2500, Training loss 0.0132, Validation loss 0.0126\n",
            "Epoch 3000, Training loss 0.0132, Validation loss 0.0126\n",
            "Epoch 3500, Training loss 0.0132, Validation loss 0.0125\n",
            "Epoch 4000, Training loss 0.0132, Validation loss 0.0125\n",
            "Epoch 4500, Training loss 0.0132, Validation loss 0.0125\n",
            "Epoch 5000, Training loss 0.0131, Validation loss 0.0125\n",
            "Epoch 500, Training loss 0.0161, Validation loss 0.0165\n",
            "Epoch 1000, Training loss 0.0156, Validation loss 0.0159\n",
            "Epoch 1500, Training loss 0.0151, Validation loss 0.0154\n",
            "Epoch 2000, Training loss 0.0148, Validation loss 0.0150\n",
            "Epoch 2500, Training loss 0.0145, Validation loss 0.0146\n",
            "Epoch 3000, Training loss 0.0143, Validation loss 0.0143\n",
            "Epoch 3500, Training loss 0.0141, Validation loss 0.0141\n",
            "Epoch 4000, Training loss 0.0140, Validation loss 0.0139\n",
            "Epoch 4500, Training loss 0.0138, Validation loss 0.0137\n",
            "Epoch 5000, Training loss 0.0137, Validation loss 0.0135\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.9994,  0.9985,  0.9999,  0.9993,  0.9962,  0.9980,  0.9966,  1.0000,\n",
              "         0.9984,  0.9990,  0.9981, -0.0363], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/My Drive/Housing.csv'\n",
        "housing = pd.DataFrame(pd.read_csv(file_path))\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "\n",
        "varlist =  ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']\n",
        "charlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "housing.pop('furnishingstatus')\n",
        "\n",
        "def binary_map(df):\n",
        "    return df.map({'yes': 1, 'no': 0})\n",
        "\n",
        "housing[charlist] = housing[charlist].apply(binary_map)\n",
        "\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "housing[varlist] = scaler.fit_transform(housing[varlist])\n",
        "\n",
        "y = housing.pop('price')\n",
        "X = housing.values\n",
        "\n",
        "t_u = torch.tensor(X)\n",
        "t_c = torch.tensor(y)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "n_samples = t_u.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "train_t_u = t_u[train_indices]\n",
        "train_t_c = t_c[train_indices]\n",
        "\n",
        "val_t_u = t_u[val_indices]\n",
        "val_t_c = t_c[val_indices]\n",
        "\n",
        "train_t_un = 0.1 * train_t_u\n",
        "val_t_un = 0.1 * val_t_u\n",
        "t_un = 0.1 * t_u\n",
        "\n",
        "def model(t_u, w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, b):\n",
        "    return w1 * t_u[:,0] + w2 * t_u[:,1] + w3 * t_u[:,2] + w4 * t_u[:,3] + w5 * t_u[:,4] + w6 * t_u[:,5] +  w7 * t_u[:,6] +  w8 * t_u[:,7] +  w9 * t_u[:,8] +  w10 * t_u[:,9] +  w11 * t_u[:,10] + b\n",
        "\n",
        "def loss_fn(t_p, t_c):\n",
        "    squared_diffs = (t_p - t_c)**2\n",
        "    return squared_diffs.mean()\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0,  1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "params.grad is None\n",
        "\n",
        "loss = loss_fn(model(t_u, *params), t_c)\n",
        "loss.backward()\n",
        "\n",
        "params.grad\n",
        "\n",
        "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
        "                  train_t_c, val_t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_t_p = model(train_t_u, *params) # <1>\n",
        "        train_loss = loss_fn(train_t_p, train_t_c)\n",
        "\n",
        "        val_t_p = model(val_t_u, *params) # <1>\n",
        "        val_loss = loss_fn(val_t_p, val_t_c)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward() # <2>\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
        "                  f\" Validation loss {val_loss.item():.4f}\")\n",
        "\n",
        "    return params\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-1\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    train_t_u = train_t_un, # <1>\n",
        "    val_t_u = val_t_un, # <1>\n",
        "    train_t_c = train_t_c,\n",
        "    val_t_c = val_t_c)\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    train_t_u = train_t_un, # <1>\n",
        "    val_t_u = val_t_un, # <1>\n",
        "    train_t_c = train_t_c,\n",
        "    val_t_c = val_t_c)\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    train_t_u = train_t_un, # <1>\n",
        "    val_t_u = val_t_un, # <1>\n",
        "    train_t_c = train_t_c,\n",
        "    val_t_c = val_t_c)\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    train_t_u = train_t_un, # <1>\n",
        "    val_t_u = val_t_un, # <1>\n",
        "    train_t_c = train_t_c,\n",
        "    val_t_c = val_t_c)"
      ]
    }
  ]
}